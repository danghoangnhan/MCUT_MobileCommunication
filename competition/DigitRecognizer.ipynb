{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 12:36:37.003605: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-01 12:36:37.466892: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'package_from_scratch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnp_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackage_from_scratch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCNNs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLeNet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LeNet\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'package_from_scratch'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from package_from_scratch.CNNs.LeNet import LeNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the training and test data\n",
    "train_df = pd.read_csv('../data/digit-recognizer/train.csv')\n",
    "test_df = pd.read_csv('../data/digit-recognizer/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# EDA\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# EDA\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split the training data into features (X_train) and labels (y_train)\n",
    "X_train = train_df.drop('label', axis=1)\n",
    "y_train = train_df['label']\n",
    "classes = train_df['label'].unique()\n",
    "X_test = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Reshape the data\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "X_test = X_test.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot encode the target variable\n",
    "y_train = to_categorical(y_train, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into features (X_train) and labels (y_train)\n",
    "X_train = train_df.drop('label', axis=1)\n",
    "y_train = train_df['label']\n",
    "X_test = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "X_test = X_test.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the target variable\n",
    "y_train = to_categorical(y_train, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train dataset into train and validation datasets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize stochastic gradient descent with learning rate of 0.005\n",
    "# how to tune learning rates ?????\n",
    "opt = SGD(lr=0.005)\n",
    "# opt = Adam(lr=0.001)\n",
    "# Instantiate AlexNet architecture\n",
    "# input image size 32x32\n",
    "# output class is 3\n",
    "LeNet_model = LeNet.build(width=28, height=28, depth=1,classes=len(classes))\n",
    "LeNet_model.summary()\n",
    "# compile the model\n",
    "# loss function: cross-entropy and optimizer: SGD\n",
    "LeNet_model.compile(loss=\"categorical_crossentropy\",\n",
    "                    optimizer=opt,\n",
    "                    metrics=[\"accuracy\"])\n",
    "no_epochs = 1000\n",
    "no_verbose = 1\n",
    "no_batch_size = 32  \n",
    "# 32 images will be presented to the network at a time,\n",
    "# and a full forward and backward pass will be\n",
    "# done to update the parameters of the network\n",
    "# train the network\n",
    "\n",
    "print(\"[INFO] training network...\")\n",
    "\n",
    "with tf.device('/gpu:1'):\n",
    "    \n",
    "    history = LeNet_model.fit(X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        batch_size=no_batch_size,\n",
    "                        epochs=no_epochs,\n",
    "                        verbose=no_verbose)\n",
    "\n",
    "    print(\"[INFO] evaluating network...\")\n",
    "\n",
    "predictions = LeNet_model.predict(X_test, batch_size=no_batch_size)\n",
    "\n",
    "print(classification_report(\n",
    "        y_val.argmax(axis=1),\n",
    "        predictions.argmax(axis=1),\n",
    "        target_names=classes\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize stochastic gradient descent with learning rate of 0.005\n",
    "# how to tune learning rates ?????\n",
    "opt = SGD(lr=0.005)\n",
    "# opt = Adam(lr=0.001)\n",
    "# Instantiate AlexNet architecture\n",
    "# input image size 32x32\n",
    "# output class is 3\n",
    "LeNet_model = LeNet.build(width=28, height=28, depth=1,classes=len(classes))\n",
    "LeNet_model.summary()\n",
    "# compile the model\n",
    "# loss function: cross-entropy and optimizer: SGD\n",
    "LeNet_model.compile(loss=\"categorical_crossentropy\",\n",
    "                    optimizer=opt,\n",
    "                    metrics=[\"accuracy\"])\n",
    "no_epochs = 1000\n",
    "no_verbose = 1\n",
    "no_batch_size = 32  \n",
    "# 32 images will be presented to the network at a time,\n",
    "# and a full forward and backward pass will be\n",
    "# done to update the parameters of the network\n",
    "# train the network\n",
    "\n",
    "print(\"[INFO] training network...\")\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    history = LeNet_model.fit(X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        batch_size=no_batch_size,\n",
    "                        epochs=no_epochs,\n",
    "                        verbose=no_verbose)\n",
    "\n",
    "    print(\"[INFO] evaluating network...\")\n",
    "\n",
    "predictions = LeNet_model.predict(X_test, batch_size=no_batch_size)\n",
    "\n",
    "print(classification_report(\n",
    "        y_val.argmax(axis=1),\n",
    "        predictions.argmax(axis=1),\n",
    "        target_names=classes\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "val_loss, val_acc = LeNet_model.evaluate(X_val, y_val)\n",
    "print('Validation Loss:', val_loss)\n",
    "print('Validation Accuracy:', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy curves\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation loss curves\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the predictions on the test set\n",
    "y_pred = LeNet_model.predict(X_test)\n",
    "\n",
    "# Convert the predictions to integer labels\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Plot some examples of the test set with their predicted labels\n",
    "num_examples = 9\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(num_examples):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_test[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel('Predicted Label: {}'.format(y_pred[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save to csv file\n",
    "submission = pd.DataFrame({'ImageId':list(range(1,len(predictions)+1)), 'Label':predictions})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv file\n",
    "submission = pd.DataFrame({'ImageId':list(range(1,len(predictions)+1)), 'Label':predictions})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions on the test set\n",
    "y_pred = LeNet_model.predict(X_test)\n",
    "\n",
    "# Convert the predictions to integer labels\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Plot some examples of the test set with their predicted labels\n",
    "num_examples = 9\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(num_examples):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_test[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel('Predicted Label: {}'.format(y_pred[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv file\n",
    "submission = pd.DataFrame({'ImageId':list(range(1,len(predictions)+1)), 'Label':predictions})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mcut] *",
   "language": "python",
   "name": "conda-env-mcut-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
